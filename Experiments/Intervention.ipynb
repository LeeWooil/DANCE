{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24226150",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8996433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CBM model with relative path (DANCE repo) ---------------------------\n",
    "from __future__ import annotations\n",
    "import argparse, json, sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def find_repo_root(marker=\"CBM_training\"):\n",
    "    cwd = Path.cwd()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if (p / marker).exists():\n",
    "            return p\n",
    "repo_root = find_repo_root(\"CBM_training\")\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "\n",
    "from CBM_training.model import cbm  # from CBM_training/model/cbm.py\n",
    "from CBM_training.model import plots\n",
    "\n",
    "# --- Configure load_dir (relative to Experiments/) ----------------------------\n",
    "cfg = argparse.Namespace()\n",
    "cfg.load_dir = Path(\"../result/Penn_Action_result/penn-action_Penn_Action_motion_label+Penn_action_object_concept+Penn_action_scene_concept\")\n",
    "\n",
    "args_path = cfg.load_dir / \"args.txt\"\n",
    "if not args_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing args.txt at: {args_path.resolve()}\")\n",
    "\n",
    "# Load saved args\n",
    "with args_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Backbone:\", getattr(args, \"backbone\", \"<unknown>\"))\n",
    "\n",
    "# Load model\n",
    "model = cbm.load_cbm_dynamic(str(cfg.load_dir), device, args)\n",
    "print(\"Model loaded successfully on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5db2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept number: 147\n",
      "['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load class list\n",
    "cls_file = Path(args.video_anno_path) / \"class_list.txt\"\n",
    "with cls_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    classes = f.read().splitlines()\n",
    "\n",
    "def load_all_concepts_by_type(load_dir: Path, train_mode: list[str]):\n",
    "    \"\"\"Load concepts.txt for each concept type in train_mode.\"\"\"\n",
    "    concept_dict = {}\n",
    "    for concept_type in train_mode:\n",
    "        concept_path = Path(load_dir) / concept_type / \"concepts.txt\"\n",
    "        if concept_path.exists():\n",
    "            with concept_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                concepts = f.read().splitlines()\n",
    "            concept_dict[concept_type] = concepts\n",
    "    return concept_dict\n",
    "\n",
    "# Example usage\n",
    "concept_dict = load_all_concepts_by_type(cfg.load_dir, args.train_mode)\n",
    "\n",
    "if len(args.train_mode) < 2:\n",
    "     # Single-mode: use the concept file of the corresponding type\n",
    "    tm = args.train_mode[0]\n",
    "    concepts = concept_dict.get(tm, [])\n",
    "\n",
    "    if \"class_attributes\" in Path(args.pose_label).name:\n",
    "        # UCF101 attributes\n",
    "        attr_path = Path(\"../dataset/UCF101/class_attributes/attribute.txt\")\n",
    "        with attr_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            concepts = f.read().splitlines()\n",
    "\n",
    "\n",
    "     # Special case: attribute + object/scene combination\n",
    "    elif \"attr_object_scene\" in str(cfg.load_dir):\n",
    "        scene_obj_path = Path(\"../dataset/UCF101/class_attributes_only_object_place/attribute.txt\")\n",
    "        with scene_obj_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            scene_object_concepts = f.read().splitlines()\n",
    "        head_len = max(0, len(concepts) - len(scene_object_concepts))\n",
    "        concepts = concepts[:head_len] + scene_object_concepts\n",
    "\n",
    "else:\n",
    "    # Multi-mode: aggregated concepts are always generated\n",
    "    agg_path = Path(cfg.load_dir) / \"aggregated\" / \"concepts.txt\"\n",
    "    with agg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        concepts = f.read().splitlines()\n",
    "\n",
    "assert len(concepts) == model.final.weight.shape[1], \"Concept count mismatch\"\n",
    "print(f\"Concept number: {len(concepts)}\")\n",
    "print(concepts[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0907c",
   "metadata": {},
   "source": [
    "## Load video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183624b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the class = 15\n",
      "Number of the class = 15\n"
     ]
    }
   ],
   "source": [
    "from CBM_training.video_dataloader import datasets\n",
    "\n",
    "# Build validation dataset (for evaluation)\n",
    "val_video_dataset, _ = datasets.build_dataset(False, False, args)\n",
    "\n",
    "# Build another dataset instance for visualization\n",
    "val_visualize_dataset, _ = datasets.build_dataset(False, False, args)\n",
    "val_visualize_dataset.visualize = True\n",
    "val_visualize_dataset.no_aug = True\n",
    "\n",
    "# Extract target labels\n",
    "val_targets = val_video_dataset.label_array\n",
    "val_y = torch.LongTensor(val_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de25faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting concept activations: 100%|██████████| 34/34 [00:58<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Saved all_activations to: ../result/Penn_Action_result/penn-action_Penn_Action_motion_label+Penn_action_object_concept+Penn_action_scene_concept/all_concept_activations.pt\n",
      "[info] all_activations shape: (1067, 147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Extract and cache concept activations for the validation set ------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# I/O config (save next to the loaded experiment directory; no hardcoded paths)\n",
    "save_path = Path(cfg.load_dir) / \"all_concept_activations.pt\"\n",
    "\n",
    "# DataLoader config (portable across OS/CPUs)\n",
    "batch_size = 32\n",
    "num_workers = min(4, (os.cpu_count() or 1))  # keep conservative for cross-platform\n",
    "pin_memory = (device == \"cuda\")\n",
    "\n",
    "if save_path.exists():\n",
    "    print(f\"[info] File already exists at: {save_path}\")\n",
    "    all_activations = torch.load(save_path, map_location=\"cpu\")\n",
    "else:\n",
    "    all_activations = []\n",
    "    loader = DataLoader(\n",
    "        val_video_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for videos, _labels in tqdm(loader, desc=\"Extracting concept activations\"):\n",
    "            videos = videos.to(device, non_blocking=pin_memory)\n",
    "            outputs, concept_act = model(videos)  # `concept_act` shape: (N, C)\n",
    "            all_activations.append(concept_act.cpu())\n",
    "\n",
    "    # Concatenate to a single (N_total, C) tensor and save\n",
    "    all_activations = torch.cat(all_activations, dim=0)\n",
    "    torch.save(all_activations, save_path)\n",
    "    print(f\"[info] Saved all_activations to: {save_path}\")\n",
    "\n",
    "print(\"[info] all_activations shape:\", tuple(all_activations.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load backbone features and classifier weights ---------\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Preconditions (these are defined in earlier cells)\n",
    "assert \"args\" in globals(), \"`args` is not defined (loaded from args.txt).\"\n",
    "assert \"cfg\" in globals() and hasattr(cfg, \"load_dir\"), \"`cfg.load_dir` is not configured.\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def resolve_backbone_feature_paths(args, repo_root: Path | None = None):\n",
    "    \"\"\"\n",
    "    Resolve train/val backbone feature file paths without hardcoded absolute paths.\n",
    "    Priority:\n",
    "      1) Use args.backbone_features if it exists.\n",
    "      2) Otherwise, infer from a conventional layout:\n",
    "         CBM_training/results/Features/{DATASET}/{BACKBONE}/\n",
    "           {DATASET}_train_{BACKBONE}.pt\n",
    "           {DATASET}_val_{BACKBONE}.pt\n",
    "    \"\"\"\n",
    "    # Try provided path in args (may be absolute or relative to CWD)\n",
    "    if hasattr(args, \"backbone_features\"):\n",
    "        p_train = Path(args.backbone_features)\n",
    "        if p_train.exists():\n",
    "            p_val = Path(\n",
    "                str(p_train).replace(f\"{args.data_set}_train\", f\"{args.data_set}_val\")\n",
    "            )\n",
    "            if not p_val.exists():\n",
    "                raise FileNotFoundError(f\"Could not infer val feature path from: {p_train}\")\n",
    "            return p_train, p_val\n",
    "\n",
    "    # Fallback: infer a conventional path within the repo\n",
    "    # Attempt to use the same repo_root detection done earlier, otherwise walk up.\n",
    "    if repo_root is None:\n",
    "        from pathlib import Path as _P\n",
    "        cwd = _P.cwd()\n",
    "        for p in [cwd, *cwd.parents]:\n",
    "            if (p / \"CBM_training\").exists():\n",
    "                repo_root = p\n",
    "                break\n",
    "        if repo_root is None:\n",
    "            raise RuntimeError(\"Cannot find repository root containing 'CBM_training'.\")\n",
    "\n",
    "    dataset = getattr(args, \"data_set\", None)\n",
    "    backbone = getattr(args, \"backbone\", None) or getattr(args, \"backbone_name\", None)\n",
    "    if dataset is None or backbone is None:\n",
    "        raise AttributeError(\"`args` must include `data_set` and `backbone` (or `backbone_name`).\")\n",
    "\n",
    "    # Example file names: UCF101_train_vmae_vit_base_patch16_224.pt\n",
    "    fname_train = f\"{dataset}_train_{backbone}.pt\"\n",
    "    fname_val   = f\"{dataset}_val_{backbone}.pt\"\n",
    "\n",
    "    # Some repos add another level (e.g., 'vmae') between dataset and file; try a few common layouts.\n",
    "    candidates = [\n",
    "        repo_root / \"CBM_training\" / \"results\" / \"Features\" / dataset / backbone / fname_train,\n",
    "        repo_root / \"CBM_training\" / \"results\" / \"Features\" / dataset / fname_train,\n",
    "    ]\n",
    "\n",
    "    p_train = next((p for p in candidates if p.exists()), None)\n",
    "    if p_train is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Backbone train features not found. Tried:\\n- \" + \"\\n- \".join(map(str, candidates))\n",
    "        )\n",
    "\n",
    "    # Derive val path alongside train\n",
    "    p_val = p_train.with_name(fname_val)\n",
    "    if not p_val.exists():\n",
    "        # Also try parallel directory forms if needed\n",
    "        alt_candidates = [\n",
    "            p_train.parent / fname_val,\n",
    "            repo_root / \"CBM_training\" / \"results\" / \"Features\" / dataset / backbone / fname_val,\n",
    "            repo_root / \"CBM_training\" / \"results\" / \"Features\" / dataset / fname_val,\n",
    "        ]\n",
    "        p_val = next((p for p in alt_candidates if p.exists()), None)\n",
    "        if p_val is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Backbone val features not found. Tried:\\n- \" + \"\\n- \".join(map(str, alt_candidates))\n",
    "            )\n",
    "\n",
    "    return p_train, p_val\n",
    "\n",
    "# Resolve feature paths (portable, no hardcoded absolute paths)\n",
    "train_feat_path, val_feat_path = resolve_backbone_feature_paths(args)\n",
    "\n",
    "# Load features to the selected device\n",
    "backbone_features = torch.load(train_feat_path, map_location=device).float()\n",
    "val_backbone_features = torch.load(val_feat_path, map_location=device).float()\n",
    "\n",
    "# --- Load classifier weights/bias and projection stats -----------------------\n",
    "load_dir = Path(cfg.load_dir)\n",
    "\n",
    "if hasattr(args, \"train_mode\") and len(args.train_mode) == 1:\n",
    "    load_sub_dir = load_dir / args.train_mode[0]\n",
    "    W_c = torch.load(load_sub_dir / \"W_c.pt\", map_location=device)\n",
    "    W_g = torch.load(load_sub_dir / \"W_g.pt\", map_location=device)\n",
    "    b_g = torch.load(load_sub_dir / \"b_g.pt\", map_location=device)\n",
    "    proj_mean = torch.load(load_sub_dir / \"proj_mean.pt\", map_location=device)\n",
    "    proj_std  = torch.load(load_sub_dir / \"proj_std.pt\", map_location=device)\n",
    "else:\n",
    "    # Aggregated (multi-mode) weights\n",
    "    agg_dir = load_dir / \"aggregated\"\n",
    "    W_g = torch.load(agg_dir / \"W_g.pt\", map_location=device)\n",
    "    b_g = torch.load(agg_dir / \"b_g.pt\", map_location=device)\n",
    "    # If you need W_c/proj stats for aggregated runs, add them here when available.\n",
    "\n",
    "print(\"[info] Loaded features and classifier parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae6c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔝 Top 5 most activated concepts across dataset:\n",
      "1. Boot camp class           | Total activation: +191.9189\n",
      "2. Workout clothing          | Total activation: +171.8232\n",
      "3. CrossFit box              | Total activation: +171.7858\n",
      "4. Resistance bands          | Total activation: +171.3524\n",
      "5. Personal training session | Total activation: +171.1149\n"
     ]
    }
   ],
   "source": [
    "# --- Analyze cumulative activations across all concepts ----------------------\n",
    "import torch\n",
    "\n",
    "# Preconditions\n",
    "assert \"all_activations\" in globals(), \"`all_activations` is not defined.\"\n",
    "assert all_activations.ndim == 2, \"Expected all_activations shape: (N, C).\"\n",
    "\n",
    "# (1) Sum activations across all samples → (C,)\n",
    "concept_activation_sum = all_activations.sum(dim=0)\n",
    "\n",
    "# (2) Find top-k concepts with largest cumulative activations\n",
    "topk_vals, topk_indices = torch.topk(concept_activation_sum, k=5)\n",
    "\n",
    "print(\"🔝 Top 5 most activated concepts across dataset:\")\n",
    "for i, idx in enumerate(topk_indices):\n",
    "    val = concept_activation_sum[idx].item()\n",
    "    # Fallback if concept names are unavailable\n",
    "    if \"concepts\" in globals() and len(concepts) > int(idx):\n",
    "        concept_name = concepts[idx] if isinstance(concepts[idx], str) else f\"Concept {idx}\"\n",
    "    else:\n",
    "        concept_name = f\"Concept {idx}\"\n",
    "    print(f\"{i+1}. {concept_name:<25} | Total activation: {val:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze per-class errors: concepts & confused classes -------------------\n",
    "import torch\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "def analyze_wrong_concepts_and_confusions(\n",
    "    class_idx: int,\n",
    "    prediction: torch.Tensor,   # (N, C) logits/probs\n",
    "    val_y: torch.Tensor,        # (N,)\n",
    "    val_c: torch.Tensor,        # (N, num_concepts) concept activations\n",
    "    concepts: List[str],\n",
    "    classes: List[str],\n",
    "    topk_concept: int = 10,\n",
    "    topk_confuse: int = 5,\n",
    "    threshold: float = 0.0,\n",
    "    sort_by: str = \"mean\",      # \"mean\" | \"ratio\" | \"absmean\"\n",
    "    return_stats: bool = False,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Print diagnostics for a given class: accuracy among its samples, the most active\n",
    "    concepts in wrong predictions, and which other classes it is most often confused with.\n",
    "\n",
    "    Args:\n",
    "        class_idx: Class index to analyze.\n",
    "        prediction: (N, C) tensor of logits or probabilities.\n",
    "        val_y: (N,) ground-truth class indices.\n",
    "        val_c: (N, num_concepts) concept activations aligned with samples in `prediction`.\n",
    "        concepts: List of concept names (len = num_concepts).\n",
    "        classes: List of class names (len = C).\n",
    "        topk_concept: How many concepts to display from wrong predictions.\n",
    "        topk_confuse: How many confused classes to display.\n",
    "        threshold: Concept is considered \"active\" if activation > threshold.\n",
    "        sort_by: Ranking for concepts among wrong predictions:\n",
    "                 - \"mean\": descending by mean activation\n",
    "                 - \"ratio\": descending by active ratio (fraction of wrong samples > threshold)\n",
    "                 - \"absmean\": descending by mean absolute activation\n",
    "        return_stats: If True, return a dictionary of computed stats instead of None.\n",
    "\n",
    "    Returns:\n",
    "        None, or a dict with stats if `return_stats=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move to CPU for consistent printing/ops\n",
    "    prediction = prediction.detach().cpu()\n",
    "    val_y = val_y.detach().cpu()\n",
    "    val_c = val_c.detach().cpu()\n",
    "\n",
    "    # Predicted labels\n",
    "    pred_labels = prediction.argmax(dim=1)\n",
    "\n",
    "    # Subset: samples of the target class\n",
    "    target_mask = (val_y == class_idx)\n",
    "    num_total = int(target_mask.sum().item())\n",
    "\n",
    "    if num_total == 0:\n",
    "        print(f\"\\nClass {class_idx} ({classes[class_idx]}): no samples in validation set.\")\n",
    "        return None\n",
    "\n",
    "    correct_mask = target_mask & (pred_labels == val_y)\n",
    "    num_correct = int(correct_mask.sum().item())\n",
    "    class_acc = num_correct / num_total if num_total > 0 else float(\"nan\")\n",
    "\n",
    "    print(f\"\\nClass {class_idx} ({classes[class_idx]}) Accuracy: \"\n",
    "          f\"{class_acc*100:.2f}% ({num_correct}/{num_total})\")\n",
    "\n",
    "    wrong_mask = target_mask & (pred_labels != val_y)\n",
    "    num_wrong = int(wrong_mask.sum().item())\n",
    "    if num_wrong == 0:\n",
    "        print(f\"No wrong predictions for class {class_idx} ({classes[class_idx]}).\")\n",
    "        if return_stats:\n",
    "            return {\n",
    "                \"class_idx\": class_idx,\n",
    "                \"class_name\": classes[class_idx],\n",
    "                \"num_total\": num_total,\n",
    "                \"num_correct\": num_correct,\n",
    "                \"num_wrong\": 0,\n",
    "                \"accuracy\": class_acc,\n",
    "                \"top_concepts\": [],\n",
    "                \"confusions\": [],\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    # Concepts among wrong predictions\n",
    "    wrong_concepts = val_c[wrong_mask]  # (num_wrong, num_concepts)\n",
    "    mean_activation = wrong_concepts.mean(dim=0)            # (num_concepts,)\n",
    "    mean_abs_activation = wrong_concepts.abs().mean(dim=0)  # (num_concepts,)\n",
    "    active_count = (wrong_concepts > threshold).sum(dim=0)  # (num_concepts,)\n",
    "    active_ratio = active_count.float() / max(num_wrong, 1) # (num_concepts,)\n",
    "\n",
    "    # Sorting strategy\n",
    "    if sort_by == \"mean\":\n",
    "        sort_scores = mean_activation\n",
    "    elif sort_by == \"ratio\":\n",
    "        sort_scores = active_ratio\n",
    "    else:  # \"absmean\"\n",
    "        sort_scores = mean_abs_activation\n",
    "\n",
    "    # Handle potential NaNs (e.g., if num_wrong=0 which we already guarded)\n",
    "    sort_scores = torch.nan_to_num(sort_scores, nan=0.0)\n",
    "\n",
    "    # Top-K concepts\n",
    "    k_concepts = min(topk_concept, sort_scores.numel())\n",
    "    top_indices = torch.topk(sort_scores, k=k_concepts, largest=True).indices.tolist()\n",
    "\n",
    "    print(f\"\\nTop {k_concepts} concepts among wrong predictions (sorted by {sort_by}):\\n\")\n",
    "    top_concepts_rows: List[Tuple[int, float, float, float]] = []  # (idx, mean, absmean, ratio)\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        cname = concepts[idx] if isinstance(concepts[idx], str) else f\"Concept {idx}\"\n",
    "        mean_val = float(mean_activation[idx].item())\n",
    "        absmean_val = float(mean_abs_activation[idx].item())\n",
    "        ratio_val = float(active_ratio[idx].item()) * 100.0\n",
    "        cnt_val = int(active_count[idx].item())\n",
    "        top_concepts_rows.append((idx, mean_val, absmean_val, ratio_val))\n",
    "        print(f\"{rank:>2}. {cname:<30} \"\n",
    "              f\"| mean: {mean_val:+.4f} | |mean|: {absmean_val:.4f} \"\n",
    "              f\"| active: {cnt_val}/{num_wrong} ({ratio_val:.1f}%)\")\n",
    "\n",
    "    # Confused classes\n",
    "    wrong_preds = pred_labels[wrong_mask]\n",
    "    unique_preds, counts = torch.unique(wrong_preds, return_counts=True)\n",
    "    pairs = sorted(zip(unique_preds.tolist(), counts.tolist()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    k_conf = min(topk_confuse, len(pairs))\n",
    "    print(f\"\\nTop {k_conf} confused classes:\\n\")\n",
    "    confusions_rows: List[Tuple[int, str, int, float]] = []  # (pred_idx, name, count, ratio%)\n",
    "    for i, (pred_c, cnt) in enumerate(pairs[:k_conf], 1):\n",
    "        name = classes[pred_c] if 0 <= pred_c < len(classes) else f\"Class {pred_c}\"\n",
    "        ratio = (cnt / num_wrong) * 100.0\n",
    "        confusions_rows.append((pred_c, name, int(cnt), float(ratio)))\n",
    "        print(f\"{i}. Predicted as {name} ({pred_c}): {cnt}/{num_wrong} times ({ratio:.1f}%)\")\n",
    "\n",
    "    if return_stats:\n",
    "        return {\n",
    "            \"class_idx\": class_idx,\n",
    "            \"class_name\": classes[class_idx],\n",
    "            \"num_total\": num_total,\n",
    "            \"num_correct\": num_correct,\n",
    "            \"num_wrong\": num_wrong,\n",
    "            \"accuracy\": class_acc,\n",
    "            \"top_concepts\": [\n",
    "                {\n",
    "                    \"concept_idx\": idx,\n",
    "                    \"concept_name\": (concepts[idx] if isinstance(concepts[idx], str) else f\"Concept {idx}\"),\n",
    "                    \"mean\": mean_val,\n",
    "                    \"absmean\": absmean_val,\n",
    "                    \"active_ratio_pct\": ratio_val,\n",
    "                }\n",
    "                for (idx, mean_val, absmean_val, ratio_val) in top_concepts_rows\n",
    "            ],\n",
    "            \"confusions\": [\n",
    "                {\n",
    "                    \"pred_class_idx\": pred_c,\n",
    "                    \"pred_class_name\": name,\n",
    "                    \"count\": cnt,\n",
    "                    \"ratio_pct\": ratio,\n",
    "                }\n",
    "                for (pred_c, name, cnt, ratio) in confusions_rows\n",
    "            ],\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85255d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect class-specific concept weights ----------------------------------\n",
    "import torch\n",
    "\n",
    "def print_class_concept_weights(model, concepts, classes, class_idx, threshold=0.1, top_k=None):\n",
    "    \"\"\"\n",
    "    Print the most influential concept weights for a given class.\n",
    "\n",
    "    Args:\n",
    "        model: Trained CBM model with `final.weight` available.\n",
    "        concepts (list[str]): List of concept names (length = num_concepts).\n",
    "        classes (list[str]): List of class names (length = num_classes).\n",
    "        class_idx (int): Index of the class to inspect.\n",
    "        threshold (float): Only show concepts with |weight| > threshold.\n",
    "        top_k (int | None): If set, only print the top-k highest-weight concepts.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = model.final.weight[class_idx]  # shape: (num_concepts,)\n",
    "    weights_np = weights.detach().cpu().numpy()\n",
    "\n",
    "    # Filter concepts by threshold\n",
    "    filtered = [(j, weights_np[j]) for j in range(len(concepts)) if abs(weights_np[j]) > threshold]\n",
    "    # Sort by absolute weight magnitude\n",
    "    sorted_filtered = sorted(filtered, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    if top_k is not None:\n",
    "        sorted_filtered = sorted_filtered[:top_k]\n",
    "\n",
    "    print(f\"\\nClass: {classes[class_idx]} (|weight| > {threshold})\")\n",
    "    for idx, w in sorted_filtered:\n",
    "        cname = concepts[idx] if isinstance(concepts[idx], str) else f\"Concept {idx}\"\n",
    "        print(f\"{cname:<30} [{w:+.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "688bf0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0 (baseball_pitch) Accuracy: 100.00% (63/63)\n",
      "No wrong predictions for class 0 (baseball_pitch).\n",
      "\n",
      "Class: baseball_pitch (|weight| > 0.1)\n",
      "a pitcher's mound              [+0.5091]\n",
      "29                             [+0.5024]\n",
      "11                             [+0.4553]\n",
      "a baseball glove               [+0.3832]\n",
      "35                             [+0.3724]\n",
      "a catcher's mitt               [+0.3440]\n",
      "a baseball uniform             [+0.2820]\n",
      "73                             [+0.2512]\n",
      "49                             [-0.2485]\n",
      "47                             [+0.2307]\n",
      "5                              [+0.2288]\n",
      "Little league field            [+0.1693]\n",
      "13                             [-0.1416]\n",
      "Open field                     [+0.1339]\n",
      "28                             [+0.1236]\n",
      "Batting cage                   [+0.1200]\n",
      "8                              [+0.1027]\n",
      "\n",
      "Class: clean_and_jerk (|weight| > 0.1)\n",
      "42                             [+1.0891]\n",
      "59                             [+0.5870]\n",
      "56                             [+0.3364]\n",
      "21                             [+0.2215]\n",
      "52                             [+0.2167]\n"
     ]
    }
   ],
   "source": [
    "# --- Get Prediction -------------------------------------------------\n",
    "val_c = all_activations.clone()\n",
    "\n",
    "cls_layer = torch.nn.Linear(val_c.shape[1],len(classes)).to(args.device)\n",
    "cls_layer.load_state_dict({\"weight\":W_g,\"bias\":b_g})\n",
    "with torch.no_grad():\n",
    "    prediction = cls_layer(val_c.cuda().detach())\n",
    "\n",
    "# --- Run analysis for two classes --------------------------------------------\n",
    "observed_class_idx = 0\n",
    "confused_class_idx = 1\n",
    "\n",
    "analyze_wrong_concepts_and_confusions(\n",
    "    class_idx=observed_class_idx,\n",
    "    prediction=prediction,\n",
    "    val_y=val_y,\n",
    "    val_c=val_c,\n",
    "    concepts=concepts,\n",
    "    classes=classes,\n",
    "    topk_concept=40,\n",
    "    topk_confuse=5,\n",
    "    threshold=0.0\n",
    ")\n",
    "\n",
    "print_class_concept_weights(model, concepts, classes, observed_class_idx)\n",
    "print_class_concept_weights(model, concepts, classes, confused_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f00127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intervene] concept = 24\n",
      "  ↓ pullup: 0.0000 -> -0.5000\n",
      "  ↑ clean_and_jerk: 0.0000 -> 0.5000\n"
     ]
    }
   ],
   "source": [
    "# --- Intervention on classifier weights ------------\n",
    "import torch\n",
    "\n",
    "def resolve_concept_index(concept_idx, concepts):\n",
    "    \"\"\"Allow indices like 24 or strings like '24' or concept names.\"\"\"\n",
    "    if isinstance(concept_idx, int):\n",
    "        return concept_idx\n",
    "    if isinstance(concept_idx, str):\n",
    "        # If it's a digit-like string, use it as numeric index\n",
    "        if concept_idx.isdigit():\n",
    "            return int(concept_idx)\n",
    "        # Otherwise treat as concept name\n",
    "        return concepts.index(concept_idx)\n",
    "    raise TypeError(\"concept_idx must be int or str.\")\n",
    "\n",
    "def intervene_and_predict(\n",
    "    val_c: torch.Tensor,     # (N, K_concept) concept activations\n",
    "    W_g: torch.Tensor,       # (C_cls, K_concept) classifier weights\n",
    "    b_g: torch.Tensor,       # (C_cls,) classifier bias\n",
    "    concepts,                # list of concept names\n",
    "    classes,                 # list of class names\n",
    "    weight_up_class_idx: int,\n",
    "    concept_idx,             # int or str ('24' or concept name)\n",
    "    margin: float = 0.5,\n",
    "    weight_down_class_idx: int | None = None,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        original_prediction: logits with original W_g/b_g\n",
    "        modified_prediction: logits after adding/subtracting margin on selected (class, concept)\n",
    "    \"\"\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_c = val_c.to(device, non_blocking=True)\n",
    "    W_g = W_g.to(device)\n",
    "    b_g = b_g.to(device)\n",
    "\n",
    "    # Resolve concept index robustly\n",
    "    cidx = resolve_concept_index(concept_idx, concepts)\n",
    "\n",
    "    # Base classifier layer\n",
    "    base_cls = torch.nn.Linear(val_c.shape[1], W_g.shape[0], bias=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        base_cls.weight.copy_(W_g)\n",
    "        base_cls.bias.copy_(b_g)\n",
    "        original_prediction = base_cls(val_c)\n",
    "\n",
    "    # Modify W_g for intervention\n",
    "    W_g_modified = W_g.clone()\n",
    "    print(f\"[intervene] concept = {concepts[cidx] if isinstance(concepts[cidx], str) else cidx}\")\n",
    "\n",
    "    # Decrease weight for a confusing class (optional)\n",
    "    if weight_down_class_idx is not None and weight_down_class_idx >= 0:\n",
    "        before = W_g_modified[weight_down_class_idx, cidx].item()\n",
    "        W_g_modified[weight_down_class_idx, cidx] = before - margin\n",
    "        after = W_g_modified[weight_down_class_idx, cidx].item()\n",
    "        print(f\"  ↓ {classes[weight_down_class_idx]}: {before:.4f} -> {after:.4f}\")\n",
    "\n",
    "    # Increase weight for the target class\n",
    "    before = W_g_modified[weight_up_class_idx, cidx].item()\n",
    "    W_g_modified[weight_up_class_idx, cidx] = before + margin\n",
    "    after = W_g_modified[weight_up_class_idx, cidx].item()\n",
    "    print(f\"  ↑ {classes[weight_up_class_idx]}: {before:.4f} -> {after:.4f}\")\n",
    "\n",
    "    # Classifier with modified weights\n",
    "    mod_cls = torch.nn.Linear(val_c.shape[1], W_g.shape[0], bias=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        mod_cls.weight.copy_(W_g_modified)\n",
    "        mod_cls.bias.copy_(b_g)\n",
    "        modified_prediction = mod_cls(val_c)\n",
    "\n",
    "    return original_prediction, modified_prediction\n",
    "\n",
    "# --- Example usage ------------------------------------------------------------\n",
    "weight_up_class_idx = 1\n",
    "weight_down_class_idx = 2\n",
    "concept_idx = \"24\"  # can be int(24), \"24\", or an actual concept name\n",
    "\n",
    "original_pred, modified_pred = intervene_and_predict(\n",
    "    val_c=val_c,                 # (N, K_concept)\n",
    "    W_g=W_g,                     # (C_cls, K_concept)\n",
    "    b_g=b_g,                     # (C_cls,)\n",
    "    concepts=concepts,\n",
    "    classes=classes,\n",
    "    weight_up_class_idx=weight_up_class_idx,\n",
    "    concept_idx=concept_idx,\n",
    "    margin=0.5,\n",
    "    weight_down_class_idx=weight_down_class_idx,\n",
    "    device=None,                 # auto-selects cuda/cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11d4ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare predictions before/after intervention ------------------\n",
    "import torch\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "def analyze_prediction_changes(\n",
    "    original_pred: torch.Tensor,   # (N, C) logits/scores before\n",
    "    modified_pred: torch.Tensor,   # (N, C) logits/scores after\n",
    "    val_y: torch.Tensor,           # (N,) ground-truth labels\n",
    "    class_names: Optional[List[str]] = None,\n",
    "    show_limit: Optional[int] = None,  # print at most this many changed samples\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Print a compact summary of changes and return counts/indices as a dict.\n",
    "    \"\"\"\n",
    "    # Argmax labels\n",
    "    original_labels = original_pred.argmax(dim=1)\n",
    "    modified_labels = modified_pred.argmax(dim=1)\n",
    "\n",
    "    # Correctness masks\n",
    "    matched_before = (original_labels == val_y)\n",
    "    matched_after  = (modified_labels == val_y)\n",
    "\n",
    "    # Change masks\n",
    "    improved_idx = (~matched_before) & matched_after\n",
    "    degraded_idx = matched_before & (~matched_after)\n",
    "    changed_idx = improved_idx | degraded_idx\n",
    "\n",
    "    # Indices\n",
    "    indices = changed_idx.nonzero(as_tuple=False).flatten()\n",
    "    total_changed = int(indices.numel())\n",
    "\n",
    "    print(f\"\\n[Changed Samples Summary] (Total changed samples: {total_changed})\\n\")\n",
    "    print(f\"{'Δ':<2} {'ID':<6} {'GT':<20} {'Before':<20} {'After':<20}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    limit = total_changed if show_limit is None else min(show_limit, total_changed)\n",
    "    for idx in indices[:limit].tolist():\n",
    "        gt = int(val_y[idx].item())\n",
    "        orig = int(original_labels[idx].item())\n",
    "        modf = int(modified_labels[idx].item())\n",
    "\n",
    "        gt_name   = class_names[gt]   if class_names and 0 <= gt   < len(class_names) else str(gt)\n",
    "        orig_name = class_names[orig] if class_names and 0 <= orig < len(class_names) else str(orig)\n",
    "        modf_name = class_names[modf] if class_names and 0 <= modf < len(class_names) else str(modf)\n",
    "\n",
    "        change_type = \"+\" if improved_idx[idx] else \"-\"\n",
    "        print(f\"{change_type:<2} {idx:<6} {gt_name:<20} {orig_name:<20} {modf_name:<20}\")\n",
    "\n",
    "    # Accuracy summary\n",
    "    original_acc = float(matched_before.float().mean().item())\n",
    "    modified_acc = float(matched_after.float().mean().item())\n",
    "\n",
    "    print(f\"\\n[Accuracy Summary]\")\n",
    "    print(f\" - Before Intervention: {original_acc*100:.2f}%\")\n",
    "    print(f\" - After  Intervention: {modified_acc*100:.2f}%\")\n",
    "    print(f\" - Change: {(modified_acc - original_acc)*100:+.2f}%\")\n",
    "\n",
    "    # Count summary\n",
    "    cnt_before  = int(matched_before.sum().item())\n",
    "    cnt_after   = int(matched_after.sum().item())\n",
    "    cnt_improve = int(improved_idx.sum().item())\n",
    "    cnt_degrade = int(degraded_idx.sum().item())\n",
    "\n",
    "    print(f\"\\n[Detailed Count Summary]\")\n",
    "    print(f\" - Correct before: {cnt_before} samples\")\n",
    "    print(f\" - Correct after : {cnt_after} samples\")\n",
    "    print(f\" - Improved      : {cnt_improve} samples\")\n",
    "    print(f\" - Degraded      : {cnt_degrade} samples\")\n",
    "\n",
    "    return {\n",
    "        \"indices_changed\": indices.tolist(),\n",
    "        \"indices_improved\": improved_idx.nonzero(as_tuple=False).flatten().tolist(),\n",
    "        \"indices_degraded\": degraded_idx.nonzero(as_tuple=False).flatten().tolist(),\n",
    "        \"accuracy_before\": original_acc,\n",
    "        \"accuracy_after\": modified_acc,\n",
    "        \"delta_accuracy\": modified_acc - original_acc,\n",
    "        \"count_correct_before\": cnt_before,\n",
    "        \"count_correct_after\": cnt_after,\n",
    "        \"count_improved\": cnt_improve,\n",
    "        \"count_degraded\": cnt_degrade,\n",
    "        \"total_changed\": total_changed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f519447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Changed Samples Summary] (Total changed samples: 0)\n",
      "\n",
      "Δ  ID     GT                   Before               After               \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "[Accuracy Summary]\n",
      " - Before Intervention: 98.03%\n",
      " - After  Intervention: 98.03%\n",
      " - Change: +0.00%\n",
      "\n",
      "[Detailed Count Summary]\n",
      " - Correct before: 1046 samples\n",
      " - Correct after : 1046 samples\n",
      " - Improved      : 0 samples\n",
      " - Degraded      : 0 samples\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "Class 0 (baseball_pitch) Accuracy: 100.00% (63/63)\n",
      "No wrong predictions for class 0 (baseball_pitch).\n"
     ]
    }
   ],
   "source": [
    "analyze_prediction_changes(original_pred.cpu(), modified_pred.cpu(), val_y.cpu(), class_names=classes)\n",
    "print()\n",
    "print(\"*\"*100)\n",
    "print()\n",
    "analyze_wrong_concepts_and_confusions(\n",
    "    class_idx=observed_class_idx,\n",
    "    prediction=modified_pred,\n",
    "    val_y=val_y,\n",
    "    val_c=val_c,\n",
    "    concepts=concepts,\n",
    "    classes=classes,  # 클래스 이름 리스트\n",
    "    topk_concept=10,\n",
    "    topk_confuse=5,\n",
    "    threshold=0.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DANCE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
